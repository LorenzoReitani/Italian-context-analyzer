# This code is used to show the integration of the two libraries:
# -GLiNER (https://huggingface.co/spaces/tomaarsen/gliner_medium-v2.1)
# -spacy(https://spacy.io/usage/spacy-101)
# in order to analyze the context of the words inside a phrase.
# The objective is to separate a sentence into single word (token) and understand what
# is the real meaning of each word inside the phrase.
# The strategy to reach this goal is firstly to use the spacy library to separate the sentence
# into tokens and retrieve the lemma of each word. Already this process helps us to understand the origin of the word,
# for example in the frase "mattia mangiò una pizzetta al ristorante" the word "mangiò" has as lemma "mangiare" and
# "pizzetta" has as lemma "pizza". This is the first step to understand the context of the word and to disambiguate
# homonymous words with different lemma, for example "canto" can have as lemma "cantare" if it is a verb
# or "canto" if it is a noun.
# The second step is to use part of speech (pos) and dependency (dep) to understand the role of the word in the phrase
# and the correlation with the other words to create a correlated context between words.
# For more info about pos and dep look here: https://universaldependencies.org/u/pos/ , https://universaldependencies.org/u/dep/
# The third step is to use the GLiNER library to understand the semantic field of the word and to add this information
# to the token.

# the following libraries are used to import the necessary functions
import spacy
from spacy import displacy
from gliner import GLiNER

# nlp is used to load the italian model from the pretrained models of spacy
# parameter: the nome of the model to load
nlp = spacy.load("it_core_news_md")

# GLiNER is a Named Entity Recognition (NER) model capable of identifying any entity type using a bidirectional
# transformer encoder (BERT-like) of type zero-shot.
# parameter: the name of the model to load
model = GLiNER.from_pretrained("DeepMount00/universal_ner_ita")

# labels are the semantic fields that GLiMer can recognize, in this case, the labels are the following
labels = ["sport", "persona", "luogo", "pianeta", "frutto", "vestiti", "festività", "automobili", "strumenti musicali",
          "cucina", "animale"]

# texts are the sentences to analyze
texts = ["tu mangia la verdura", "noi mettiamo tutti i giocattoli nella cesta ",
         "quando tu attraversi la strada stai attento alle macchine",
         "luigi ha vinto una gara di pesca sul lago mangiando una pesca matura",
         "per preparare la pizza bisogna prendere la farina, l'acqua, il sale, il lievito e mescolare tutto in una bacinella"]


# the class Token is defined in the following way and it is used to store the information of each token in order to
# understand the context
class Token:

    # the constructor of the class Token
    # parameter:
    # index_word: the index of the word in the sentence
    # index_char: the index of the first char in the sentence
    # text: the text of the word
    # pos: the part of speech of the word
    # dep: the dependency of the word
    # lemma: the lemma of the word
    # context: the word mainly related to the token
    # head: the father of the dependency tree

    def __init__(self, index_word, index_char, text, pos, dep, lemma, context, head):
        self.index_word = index_word
        self.index_char = index_char
        self.text = text
        self.pos = pos
        self.dep = dep
        self.lemma = lemma
        self.context = context
        self.head = head
        self.semantic_field = None


# the function phrase_analyzer is used to analyze the phrase and to store the information of each token in a list
# parameter: the document (generated by the spaCy model) to analyze
def phrase_analyzer(document):
    tokens = []
    for t in doc:
        tokens.append(Token(t.i, t.idx, t.text, t.pos_, t.dep_, t.lemma_, t.lemma_, t.head))
    tokens = create_context(tokens)
    return tokens


# the function create_context is used to create the little context, strongly related to the phrase,
# of each token according to logical analysis
# parameter: the list of tokens to analyze
def create_context(tokens):
    for tok in tokens:
        if tok.dep == "nmod" or tok.dep == "amod" or tok.dep == "advmod":
            tokens[tok.head.i].context = (tokens[tok.head.i].text, tok.text)
    return tokens


# the function add_labels is used to add the semantic field to each token
# parameters:
# tokens: the list of tokens to analyze
# my_entities: the entities recognized by the GLiNER model
def add_labels(tokens, my_entities):
    for my_token in tokens:
        for my_entity in my_entities:
            if my_token.text in my_entity["text"] and my_token.index_char == my_entity["start"]:
                my_token.semantic_field = my_entity["label"]
    return tokens


# the following code is used to analyze sentences and create the context of each token
for text in texts:
    print(text + "\n")

    entities = model.predict_entities(text, labels, threshold=0.5)
    for entity in entities:
        print(entity["text"], "=>", entity["label"])
    print()
    doc = nlp(text)
    analysis = phrase_analyzer(doc)
    analysis = add_labels(analysis, entities)
    for token in analysis:
        print(token.text, token.pos, token.dep, token.lemma, token.context, token.semantic_field)

    print("####################################################\n\n")

# from the output of the code we can see that the model from spaCy is able to corrctly separate the sentence
# into tokens and retrieve the lemma of each word. When the sentence is complex enough, we can manage to
# create a correlated context between words. The GLiNER model is easy to implements,
# weighs little and does a good work in identifying the semantic field
# but is clear that id does not work perfectly in all cases. To obtain a better result, we can train a new model in
# recognizing the homonymous words with same lemma and disambiguate them.
